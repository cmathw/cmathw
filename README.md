### Hi there ðŸ‘‹

I'm currently conducting research in mechanistic interpretability.

[![LinkedIn](https://img.shields.io/badge/-LinkedIn-blue?style=flat&logo=LinkedIn&logoColor=white)](https://www.linkedin.com/in/chrismathwin/)

#### OSS Contributions:

[![](https://img.shields.io/github/issues-search?label=neelnanda-io/TransformerLens%20PRs&query=is%3Apr+is%3Aclosed+author%3Acmathw+repo%3Aneelnanda-io%2FTransformerLens)](https://github.com/neelnanda-io/TransformerLens/pulls?q=is%3Apr+is%3Aclosed+author%3Acmathw) 
[![](https://img.shields.io/github/stars/neelnanda-io/TransformerLens?style=flat&label=stars&color=yellow)](https://github.com/neelnanda-io/TransformerLens/pulls?q=is%3Apr+is%3Aclosed+author%3Acmathw)

[![](https://img.shields.io/github/issues-search?label=huggingface/transformers%20PRs&query=is%3Apr+is%3Aclosed+author%3Acmathw+repo%3Ahuggingface%2Ftransformers)](https://github.com//huggingface/transformers/pulls?q=is%3Apr+is%3Aclosed+author%3Acmathw) 
[![](https://img.shields.io/github/stars/huggingface/transformers?style=flat&label=stars&color=yellow)](https://github.com/huggingface/transformers/pulls?q=is%3Apr+is%3Aclosed+author%3Acmathw)

#### Research:

Past papers and posts that I have contributed on include:

  -  [Gated Attention Blocks: Preliminary Progress toward Removing Attention Head Superposition](https://www.lesswrong.com/posts/kzc3qNMsP2xJcxhGn/gated-attention-blocks-preliminary-progress-toward-removing-1) (code [here](https://github.com/cmathw/gated-attn))
  -  [Structured World Representations in Maze-Solving Transformers](https://arxiv.org/abs/2312.02566) (Accepted to NeurIPS UniReps '23 [Workshop](https://unireps.org/publication/))
  -  [Polysemantic Attention Head in a 4-Layer Transformer](https://www.alignmentforum.org/posts/nuJFTS5iiJKT5G5yh/polysemantic-attention-head-in-a-4-layer-transformer)
  -  [A Configurable Library for Generating and Manipulating Maze Datasets](https://arxiv.org/abs/2309.10498)

<!--
Thank you to thejaminator for OSS badge formatting
-->
